# -*- coding: utf-8 -*-
"""notebook2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MumZADL_p29CW1VGUSljMLluf71hLJ5g
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import os
import glob
import matplotlib
import pandas as pd 
import matplotlib.image as mpimg
import numpy as np
import imageio as im
from matplotlib import pyplot as plt
from keras import models
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Dropout
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

from PIL import Image
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/RMIT Deep Learning Dataset/datamodifiedname.zip"
!unzip -qq "/content/drive/My Drive/RMIT Deep Learning Dataset/datamodifiedname.zip"

!ls

!ls "/content/drive/My Drive/RMIT Deep Learning Dataset/datamodified.zip"
!unzip -qq "/content/drive/My Drive/RMIT Deep Learning Dataset/datamodified.zip"

!rm -r '/content/datamodifiedname'

!rm -r '/content/datafolder/datamodified'

cp -r '/content/datamodifiedname' '/content/foldernew'

check = os.listdir('/content/foldernew/datamodifiedname')
check
#check.sort()
#len(check)
#!rm -f '/content/foldernew/datamodifiedname/.DS_Store'

foo = "/content/datamodifiedname/4639666.jpeg"
img = mpimg.imread(foo)
plt.imshow(img)

img = image.load_img(foo, target_size=(128,128,1), color_mode="grayscale")
plt.imshow(img)

plt.imshow(img, cmap=plt.cm.gray)

imagesfoo = os.listdir('/content/datamodifiedname')
imagesfoo = ["".join(['/content/datamodifiedname/', filename]) for filename in imagesfoo]

mylist = []

for imagefoo in imagesfoo:
  img = image.load_img(imagefoo)
  img = image.img_to_array(img)
  mylist.append(img)

labels = pd.read_csv('/content/drive/My Drive/RMIT Deep Learning Dataset/property-labels.csv')
labels.set_index('property_id', inplace = True)
labels = pd.DataFrame(labels['property_type'].replace("flat", 0).replace("house", 1))
labels.shape

less_labels = labels
house = less_labels[less_labels["property_type"] == 1]
flat = less_labels[less_labels["property_type"] == 0]
labels_house_flat = pd.concat([house, flat])
labels_house_flat.shape

#not_house_flat = less_labels[~less_labels.property_type.isin([0, 1])]
#not_house_flat.shape

labels

labels_house_flat

sortedlabels_house_flat = labels_house_flat.sort_values("property_id")
sortedlabels_house_flat.reset_index(inplace=True)
sortedlabels_house_flat

sortedlabels_house_flat['property_id'] = sortedlabels_house_flat['property_id'].astype(str)
#sortedlabels_house_flat['property_id'] = sortedlabels_house_flat['property']
sortedlabels_house_flat['property_id'] = sortedlabels_house_flat['property_id'] + '.jpeg'
sortedlabels_house_flat

sortedlabels_house_flat['property_type'] = sortedlabels_house_flat['property_type'].astype(str)
sortedlabels_house_flat['property_type']

#sortedlabels_house_flat.set_index('property_id', inplace = True)
#labels.set_index('property_id', inplace = True)
#sortedlabels_house_flat.reset_index(inplace=True)
sortedlabels_house_flat

sortedlabels = sortedlabels_house_flat['property_type'].values
sortedlabels

sortedlabels_reshape = sortedlabels.reshape(-1,1)
sortedlabels_reshape

folder = '/content/foldernew/datamodifiedname'

#sortedlabels_house_flat['prop']

#train_df = pd.read_csv(sortedlabels_house_flat)
#valid_df = pd.read_csv(sortedlabels_house_flat)

train_datagen = ImageDataGenerator(rescale = 1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=sortedlabels_house_flat, 
    directory=folder, 
    x_col='property_id', 
    y_col='property_type', 
    target_size=(128, 128), 
    batch_size=32, 
    color_mode='grayscale', 
    class_mode='binary',
    subset='training',
    validate_filenames=False)

validation_generator = train_datagen.flow_from_dataframe(
    dataframe=sortedlabels_house_flat, 
    directory=folder, 
    x_col='property_id',
    y_col='property_type',
    target_size=(128, 128), 
    batch_size=32, 
    color_mode='grayscale', 
    class_mode='binary',
    subset='validation',
    validate_filenames=False)



# train_generator = train_datagen.flow_from_directory(
#         folder,
#         target_size=(150, 150),
#         batch_size=32)
#         #class_mode='binary')

# validation_generator = train_datagen.flow_from_directory(
#         folder,
#         target_size=(150, 150),
#         batch_size=32)
#         #class_mode='binary')

train_generator.shape

images = os.listdir('/content/datafolder/datamodified')
images = ["".join(['/content/datafolder/datamodified/', filename]) for filename in images]

images

indices = [int(filename) for filename in [filename.split("~")[0] for filename in [filename.split("/")[4] for filename in images]]]

list.sort(indices)
indices

str1 = ','.join(str(e) for e in indices)

str1

# mylist = []
 
# for imagefoo in images:
#     img = image.load_img(imagefoo, target_size=(128,128,1), color_mode="grayscale")
#     img = image.img_to_array(img)
#     mylist.append(img)

!for i in $(ls -1 "/content/datafolder/datamodified/"; do mv $i ${i/~*/}.jpeg; done
#!ls /content/datafolder/datamodified/

#len(mylist)
indices_array = np.array(indices)
indices_array.shape

type(indices_array)

sortedlabels_reshape

X_train_array = np.array(X_train)
X_train_array

check3 = os.listdir('/content/datafolder/datamodified')

model = Sequential()
# model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',input_shape=(128,128,1)))
# model.add(Conv2D(64, (3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(3, 3)))
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(2, activation='sigmoid'))

model.add(Conv2D(128, (3, 3), padding='same', activation='relu',input_shape=(128,128,1)))
model.add(Conv2D(128, (3, 3), activation='relu'))
#model.add(MaxPooling2D(pool_size=(1, 1)))
model.add(Dropout(0.5))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Dropout(0.5))

model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])
model.summary()

checkpointer = ModelCheckpoint(filepath="best_weights.hdf5", 
                               monitor = 'val_acc',
                               verbose=1, 
                               save_best_only=True)

#history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))
history = model.fit_generator(train_generator,
                                   steps_per_epoch = 100,
                                   epochs = 10,
                                   callbacks=[checkpointer],
                                   validation_data = validation_generator,
                                   validation_steps = 50)

#ls '/content/foldernew/datamodified'
import keras
keras.__version__

model.save('/content/my_model.h5')

from keras.models import load_model
model = load_model('/content/my_model.h5')
model

dir(model)

